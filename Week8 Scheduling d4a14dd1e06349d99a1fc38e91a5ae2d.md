# Week8 Scheduling

Created: August 5, 2022 1:31 PM
Reviewed: No

## ****Kernel-Level Threads****

1. What is Cooperative multitasking?
    - Applications must be implemented as coroutines
    - Applications must know each other
    - Applications must activate each other
    - Example
        
        ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled.png)
        
        - Operating-system primitives
            - schedule (indirectly) starts the first application thread, does not return
            - resume switches from one application thread to the next
            - The kickoff coroutine:
                - runs once for each application thread
                - activates the respective application through an upcall
2. What is preemptive multi tasking /Kernel-Level Threads
    
    Perceive “cooperation capability” as an operating-system responsibility and run applications “unnoticeably” as independent threads  by introducing the preemption mechanism
    
    - OS takes care of creating coroutine control flows (every application is implemented as a coroutine)
    - OS takes care of suspending running coroutine control flows
    - OS takes care of selecting the next coroutine control flow
3. How to achieve “Run applications “unnoticeably” as independent threads” ?
    - One OS coroutine per application
    - Application is activated by being called
    - Coroutine switch: indirect by system call / time interrupt
4. Advantages:
    - Independent application development
    - Central scheduler implementation
    - Use CPU efficiently
5. Preemptive Thread Switch example
    
    ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%201.png)
    
    - Forced CPU removal via timer interrupt, the timer handler routine can call resume to switch to another coroutine
    - **Q: What will be saved on stack when interrupt happens?**
        - For interrupt handling, the CPU automatically save the PC and status register into app1 stack and jump to wrapper function. The wrapper function save the volatile registers on the stack.  Then, wrapper calls guardian function. Here we assume the guardian calls resume directly. In the resume, before it switch the stack, the non-volatile register and the value in stack pointer must be saved. Note in the description above, we didn’t include the contents automatically saved by compiler based on calling conventions.
            
            ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%202.png)
            
    - **Q:  Why this model doesn’t work?**
        
        The OS bookkeep the ready thread in the list. In Preemptive multi tasking, both L0 thread and L1 interrupt handler can access the list through resume(). Thus, we have to do synchronization on the data structure. 
        
6. **Thread Switch in the Epilogue** 
    
    Implementation
    
    - Scheduler data (list of ready threads) reside on the epilogue level
    - All system functions that manipulate these data must acquire the epilogue lock before (enter/leave)
    - Create thread, terminate thread, voluntary thread switch, …
    
    Basic rule for thread switches:
    
    - the yielding thread requests the lock (e.g. implicitly in interrupt handling)
    - the activated thread must release the lock
    
    Corner case:
    
    - Basic rule (see above) also holds for the first thread activation(!), See:
    - In kickoff:
        
        Wacht's epilogue calls resume(), which will call kickoff eventually. However Wacht's epilogue is in critical section  and it will never return to it's caller relay(), therefore we need to leave the critical section explicitly by calling guard.leave();
        
    - In main
        
        application is the very first thread, scheduler launch this thread by call kickoff function. In kickoff, we leave the critical section, which means we have to enter critical section here
        

# Scheduling

![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%203.png)

## Classification (scheduling decision is made by:)

### ****Resource type of the scheduled hardware resource****

- CPU scheduling
    - more process than cpu→multiplexed CPU for more than 1 P
- I/O scheduling of the resource “device”, particularly “disk
    - Device-specific scheduling of I/O jobs generated by processes
    - Device parameters and device state determine the next I/O operation
    - Scheduling decisions possibly not conforming to CPU scheduling (process with high priority cpu may end up in waiting list for long time.)

### ****Operation mode of the controlled computer system (取决与程序运行的方式)****

- Batch scheduling(in 50-60s)
    - for interaction-less programs
    - **Q: why Batch scheduling?**
        - **non-preemptive scheduling (or preemptive scheduling with long time slices) → overhead of Context-switch minimization**
- Interactive scheduling
    - for ineractiv Process
    - **Event-driven, preemptive** scheduling with short time slices
    - Partly response-time minimization by heuristics
- Real-time scheduling
    - Event- or time-driven **deterministic** scheduling (需要再 deadline 之前完成 task)
    - Guarantee of keeping environment-specific deadlines(e.g brake of the car )
    - Focus: **Timeliness (keep deadline)**, not performance

### ****Point in time (做出 scheduling 的时间点)****

- Online scheduling
    - dynamic, **during** actual program execution
- Offline scheduling
    - static, **before** actual program execution

### Determinism

- Deterministic scheduling of known, exactly pre-computed processes
    - Process runtimes and deadlines are known, possibly calculated offline
    - Exact prediction of CPU load is possible
    - System guarantees and enforces process runtimes/deadlines
    - Time guarantees are valid regardless of system load
- Probabilistic scheduling of unknown processes
    - Process runtimes and deadlines are unknown
    - CPU load can only be estimated
    - System cannot give and enforce time guarantees
        - Timing guarantees  is conditionally achievable by application mechanisms

### Cooperation Behavior

- Cooperative scheduling of interdependent processes
    - Processes must **voluntarily yield the CPU** in favor of other processes by using system call
    - **System calls** must (directly/indirectly) activate the **scheduler**
- Preemptive scheduling of independent processes
    - Processes are **forcibly deprived of the CPU** in favor of other processes
    - Events(interrupt) can trigger preemption of the running process

## Computer Architecture

- **Uni-processor scheduling**
    - **pseudo** parallel for multiprogramming/processing systems
- Multi-processor scheduling in shared-memory systems
    - Parallel process execution possible
    - All CPU process share a **global ready list**
        - Pro
            - Better Load balancing
        - con
            - need to do synchronization on ready list as multiple cpu may want to access it
            - low cache utilization as Processes are not bound to particular CPUs
    - Each processor has its **local ready list**
        - Pro:
            - Better cache utilization
            - Less synchronization costs
        - Con:
            - Need to do load balancing in case  a CPU’s ready list is empty
    
    **Q: Pro and Con for the 2 variant ready list**
    

## Scheduling Criteria

![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%204.png)

![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%205.png)

## Scheduling in windows

1. Process and thread
    - each thread has its own set of regs and stack, all threads have common code segment and data segment
    - Process: Environment and address space for threads
2. The Scheduler (Scheduler assigns processing time to threads)
    - Preemptive, priority-based scheduling
        - high-priority thread preempts thread with lower priority regardless whether thread currently in user or kernel mode
        - Round-Robin for threads with same priority
            - Round-robin assignment of one time slice (“Quantum”)
    - Thread priorities
        - 0 to 31, subdivided in three ranges
            - Variable Priorities: 1 to 15
            - Real-time Priorities: 16 to 31
            - Priority 0 is reserved for the Zero-Page Thread
        
        Note: Most functionality of the Executive (“kernel”) implemented as threads, Threads of the Executive maximally use priority 23
        
    - Time Slice (Quantum)
        
        ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%206.png)
        
        - Quantum is decreased
        - System has 2 different mode of Quantum (fixed and variable). In variable mode, frontend thread and backend thread has different quantum
        
    - Priority Classes, Relative Thread Priority
        
        Each process has its own process priority class. The process may have multi. thread. Therefore in the class, each thread may have different relative thread priority. The final priority of a thread is determined by its process priority class and its relative thread priority
        
        ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%207.png)
        
    - Variable Priorities （1-15）
        - Scheduler use variant strategies to prioritize “important” threads
            - Quantum Stretching : frontend thread get long quantum, while backend thread get short quantum
            - Dynamic priority boost:   The system dynamically raises thread priorities in specific situations (keyboard input etc)
                
                ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%208.png)
                
        - Progress guarantee: each thread will get some work done in the duration of time
            - Every 3 to 4 seconds, up to 10 “disadvantaged” threads are raised to priority 15 for two time slices
        - Thread priority is calculated using this (simplified) formula:
            - Process priority class + Thread priority + Dynamic priority boost
    - : Realtime Priorities (16 - 31)
        - Pure priority-based Round-Robin
            - No progress guarantee
            - No dynamic boost
            - Operating system itself can be negatively affected
            - Special user privilege necessary (SeIncreaseBasePriorityPrivilege)
        - Thread priority is calculated using this formula:
            - REALTIME_PRIORITY_CLASS + Thread priority
    - Selecting the Next Thread (SMP)
        - goal is to achieve ”fair” Round-Robin at maximum throughput.
        - Problem: Cache effects
        - base on Affinity (mapping of CPUs to thread):
            - `hard_affinity`:  fixed mapping between a thread and a CPU
            - `ideal_processor`: if possible i want to run this Thread on CPU N
            - `soft_affinity`:  Previous CPU the thread ran on
            - `last_run` : Point in time the thread ran last (thread 上次 运行的时间点)
        - **Q:Algorithm**
            
            ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%209.png)
            
            ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2010.png)
            
    - Changes in Windows 2003
        
        ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2011.png)
        
    - Conclusion
        - „interactive, probabilistic, online, preemptive, multi-processor CPU scheduling“
        - Priority model allows fine-grained CPU-time allocation
            - Dynamic modifications
            - User-mode threads with high real-time priorities take precedence over all system threads!
            - Threads in the Executive (kernel) are generally preemptible
        - Further SMP improvements in Windows 2003

## Scheduling in Linux

1. **Linux Tasks …**
    - are the Linux-Kernel abstraction for …
        - UNIX processes: one thread in one address space
        - Linux Threads: special process that shares its virtual address space with at least one other thread
    - are the activities considered by the scheduler
2. 2 level ****Scheduler:****
    
     ****sched_rt (real time scheduler) + sched_fair (try to run each thread fairly), sched_rt  has higher priority than sched_fair.****
    
    ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2012.png)
    
    ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2013.png)
    
    ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2014.png)
    
    ![Untitled](Week8%20Scheduling%20d4a14dd1e06349d99a1fc38e91a5ae2d/Untitled%2015.png)